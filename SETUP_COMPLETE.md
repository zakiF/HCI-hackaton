# âœ… Hackathon Setup Complete!

## What Has Been Done

All files and structure for the 7-hour LLM learning hackathon are ready.

---

## ğŸ“¦ Created Files

### **Main Chatbot Files**
- âœ… `app/python/chatbot.py` - Main chatbot with feature flags
- âœ… `app/python/chatbot_base.py` - Backup version

### **Module Skeletons** (One per person/team)
- âœ… `modules/R/multi_gene_viz.R` - Zaki + Udhaya
- âœ… `modules/python/llm_filters.py` - Qing
- âœ… `modules/python/conversation.py` - Miao
- âœ… `modules/python/llm_stats.py` - Tayler
- âœ… `modules/python/llm_rag.py` - David

### **Shared Utilities**
- âœ… `utils/python/ollama_utils.py` - Python LLM helpers
- âœ… `utils/R/ollama_utils.R` - R LLM helpers

### **Documentation**
- âœ… `HACKATHON_README.md` - Quick start guide
- âœ… `docs/HACKATHON_OVERVIEW.md` - Goals and learning objectives
- âœ… `docs/TASK_ASSIGNMENTS.md` - Detailed tasks per person
- âœ… `docs/HACKATHON_SCHEDULE.md` - 7-hour timeline
- âœ… `docs/GIT_WORKFLOW.md` - Branch strategy and Git guide
- âœ… `docs/OLLAMA_SETUP.md` - Installation instructions

### **Git Branches**
- âœ… `feature/multi-gene` - Zaki + Udhaya
- âœ… `feature/filters` - Qing
- âœ… `feature/conversation` - Miao
- âœ… `feature/stats` - Tayler
- âœ… `feature/rag` - David

---

## ğŸ¯ What Each File Does

### **Chatbot Files**

**`chatbot.py`** (Main version):
- Feature flags for each person's work
- Safe imports with try-except
- Placeholders showing where each person's code plugs in
- Routes: RAG â†’ Stats â†’ Plotting (with filters and conversation)

**`chatbot_base.py`** (Backup):
- Simple working chatbot with no experimental features
- Run this if integration fails completely
- Guaranteed to work for demo

### **Module Files**

Each module file contains:
- Function stubs with TODOs for the person
- Working example code they can use/modify
- Test cases at the bottom
- Comments explaining LLM learning goals
- Can be run standalone for testing

**Example structure**:
```python
# Main functions (person implements these)
def extract_something(user_input):
    # TODO: Write LLM prompt here
    pass

# Hardcoded helper functions (provided)
def apply_something(data, params):
    # Implementation provided
    pass

# Testing section
if __name__ == "__main__":
    # Test cases to run standalone
    pass
```

### **Utility Files**

**`utils/python/ollama_utils.py`**:
Provides shared functions everyone can use:
- `call_ollama()` - Basic LLM calling
- `call_ollama_json()` - Get JSON responses
- `ask_llm_to_classify()` - Classification helper
- `ask_llm_to_extract()` - Extraction helper
- `demonstrate_llm_skills()` - Shows examples of the 4 skills

**`utils/R/ollama_utils.R`**:
R version of the same, for Zaki + Udhaya

---

## ğŸ“š Documentation Summary

### **HACKATHON_README.md** (Start here!)
- Quick overview and checklist
- Pre-hackathon setup steps
- Team assignments table
- Quick tips for LLM calling

### **HACKATHON_OVERVIEW.md** (Main goals)
- The 4 core LLM skills everyone learns
- Detailed explanation of each person's task
- Learning philosophy
- Demo structure

### **TASK_ASSIGNMENTS.md** (Detailed instructions)
- Step-by-step implementation guide per person
- Code examples
- Testing strategies
- Time management tips
- "Tips for [Person]" sections

### **HACKATHON_SCHEDULE.md** (Timeline)
- Hour-by-hour schedule for 7 hours
- Milestones for each session
- Integration plan (Day 2 11am)
- Success criteria and checklists

### **GIT_WORKFLOW.md** (Collaboration)
- Branch strategy explained
- How to commit and push
- Integration process
- Troubleshooting Git issues
- Cheat sheet of common commands

### **OLLAMA_SETUP.md** (Installation)
- How to install Ollama
- How to download llama3.2
- Testing instructions
- Troubleshooting common issues
- Day-of startup checklist

---

## ğŸ“ The Learning Structure

### **4 Core LLM Skills** (Everyone Learns):

1. **Intent Classification**
   - Categorizing user input
   - Example: plot vs stats vs question

2. **Parameter Extraction**
   - Pulling specific info from natural language
   - Example: gene names, filter conditions, test parameters

3. **Natural Language Generation**
   - Converting data to readable explanations
   - Example: Explaining stats results, gene functions

4. **Structured Output**
   - Getting LLM to return JSON/structured data
   - Example: `{"gene": "TP53", "plot_type": "violin"}`

### **How Each Person Practices These**:

**Zaki + Udhaya** (Multi-gene viz):
- âœ… Extraction (multiple gene names)
- âœ… Classification (plot type selection)
- âœ… Structured output (JSON list of genes)

**Qing** (Filters):
- âœ… Extraction (filter conditions)
- âœ… Structured output (filter params as JSON)
- âœ… Classification (detect filtering intent)

**Miao** (Conversation):
- âœ… Classification (follow-up vs new query)
- âœ… Extraction (resolve "it", "that" references)
- âœ… Context management

**Tayler** (Stats):
- âœ… Classification (stats vs plot)
- âœ… Extraction (gene, test, groups)
- âœ… Generation (explain results)
- âœ… Structured output (test params as JSON)

**David** (RAG):
- âœ… Classification (question vs plot)
- âœ… Extraction (gene from question)
- âœ… Generation (explain gene function)
- âœ… RAG pattern (retrieval + generation)

---

## ğŸš€ How It All Works Together

### **User Query Flow**:

```
User input: "Show TP53 in tumor samples only"
    â†“
[Route 1 Check] David's RAG: Is this a gene question? â†’ No
    â†“
[Route 2 Check] Tayler's Stats: Is this a stats query? â†’ No
    â†“
[Route 3: Plotting]
    â”œâ”€ Miao's Conversation: Resolve context â†’ "Show TP53 in tumor samples only" (no change)
    â”œâ”€ Base: Extract gene â†’ "TP53"
    â”œâ”€ Qing's Filters: Extract filter â†’ {condition: "Primary Tumor", exclude: false}
    â”œâ”€ Apply filter â†’ subset data
    â””â”€ Zaki+Udhaya: Create plot â†’ boxplot of TP53 in tumor samples
```

### **Feature Flags Control Integration**:

```python
# In chatbot.py
ENABLE_RAG = True            # David's feature
ENABLE_STATS = True          # Tayler's feature
ENABLE_FILTERS = True        # Qing's feature
ENABLE_CONVERSATION = True   # Miao's feature
ENABLE_MULTI_GENE = True     # Zaki+Udhaya's feature

# If integration breaks:
ENABLE_RAG = False  # Disable broken feature
# Chatbot still works with other features!
```

---

## ğŸ“… Timeline Quick Reference

### **Pre-Hackathon** (Before Day 1):
- [ ] Everyone installs Ollama
- [ ] Everyone downloads llama3.2
- [ ] Everyone tests their setup
- [ ] Everyone reads HACKATHON_README.md

### **Day 1 Morning** (10am-12pm):
- 10:00-10:30: LLM workshop (all together)
- 10:30-10:45: Task assignment review
- 10:45-12:00: Start individual work

### **Day 1 Afternoon** (3pm-5pm):
- Continue feature development
- Test with different inputs
- Iterate on prompts

### **Day 2 Morning** (9am-12pm):
- 9:00-11:00: Finish features
- 11:00-12:00: **Integration** (critical!)

### **Day 2 Afternoon** (2pm-3pm):
- Demo prep and rehearsal

### **Day 2 3pm**:
- **Presentation!**

---

## ğŸ¯ Success Metrics

### **Technical**:
- âœ… Base chatbot works
- âœ… Each person's module works standalone
- âœ… 3-5 features integrated (not all required)

### **Learning**:
Each person can answer:
- âœ… How do I call a local LLM?
- âœ… How do I write prompts for extraction/classification?
- âœ… What are LLM limitations?
- âœ… Where would I use LLMs in my research?

### **Demo**:
- âœ… 10-15 minute presentation
- âœ… Live demo of features
- âœ… Explanation of LLM concepts learned

---

## ğŸ› ï¸ Testing Before Hackathon

### **You should test**:

1. **Ollama works**:
   ```bash
   ollama serve
   # In another terminal:
   ollama run llama3.2 "What is 2+2?"
   ```

2. **Base chatbot works**:
   ```bash
   streamlit run app/python/chatbot_base.py
   ```

3. **Python utils work**:
   ```python
   from utils.python.ollama_utils import call_ollama
   response = call_ollama("Test")
   print(response)
   ```

4. **R utils work** (for Zaki+Udhaya):
   ```r
   source("utils/R/ollama_utils.R")
   response <- call_ollama("Test")
   cat(response)
   ```

---

## ğŸ“¦ What to Send to Team

### **Email to Team**:

**Subject**: Hackathon Setup - Action Required Before [Day 1 Date]

**Body**:
```
Hi team,

The hackathon structure is ready! Please complete these steps BEFORE we start:

1. **Install Ollama**: https://ollama.com/
   - Download and install for your OS
   - Run: ollama pull llama3.2

2. **Clone the repo**:
   git clone [REPO URL]
   cd HCI-hackaton

3. **Checkout your branch**:
   - Zaki + Udhaya: git checkout feature/multi-gene
   - Qing: git checkout feature/filters
   - Miao: git checkout feature/conversation
   - Tayler: git checkout feature/stats
   - David: git checkout feature/rag

4. **Install packages**:
   - Python: pip install streamlit pandas matplotlib requests scipy
   - R: install.packages(c("shiny", "httr", "jsonlite", "tidyverse", "ggplot2"))

5. **Read the docs** (in this order):
   - HACKATHON_README.md (quick overview)
   - docs/HACKATHON_OVERVIEW.md (goals and learning objectives)
   - docs/TASK_ASSIGNMENTS.md (your specific task)
   - docs/OLLAMA_SETUP.md (setup help if needed)

6. **Test everything works**:
   - ollama serve (keep running)
   - streamlit run app/python/chatbot_base.py

If you have ANY issues with setup, post in Slack NOW so we can help!

See you [Day 1 Date] at 10am!

Zaki
```

---

## ğŸ‰ You're Done!

Everything is ready for a successful LLM learning hackathon.

### **Key Files for You to Review**:
1. `HACKATHON_README.md` - Send this to your team
2. `docs/HACKATHON_OVERVIEW.md` - Make sure goals match your vision
3. `docs/TASK_ASSIGNMENTS.md` - Review each person's assignment

### **Next Steps**:
1. Review all documentation
2. Test the base chatbot works
3. Send setup email to team
4. Confirm everyone can access the repo
5. Day of: Make sure everyone's Ollama is running!

---

**Ready to run a great hackathon!** ğŸš€ğŸ§¬ğŸ¤–
